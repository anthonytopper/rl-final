# A Comparative Study of Jitter-Mitigating Methods for Deep Reinforcement Learning
## Abstract:

A Deep Q Learning (DQN) agent, trained based on exploration on the environment and aiming to maximize reward from the states, usually introduces unstable and unnecessary actions and make 'jittering' performance. In many cases, people can easily distinguish whether a performance is acted by a human player or a computer player. In this project, our goal is to prevent unnecessary actions and make the agent perform smoothly. To achieve our goal, we proposed adding bias into the cost function and adding penalties for unnecessary actions. To better visualize our work, we made comparison experiments, on classic games like Flappy Bird and Super Mario.
For the report, see https://github.com/anthonytopper/rl-final/blob/main/paper.pdf

### Team Members: 
Ajay Tulsyan, Anthony Topper, Apiwat Ditthapron, Caner Tol, and Xiaosong Wen

## Branches:
### Flappy Bird:
https://github.com/anthonytopper/rl-final/tree/Flappy

https://github.com/anthonytopper/rl-final/tree/task/flappy
### Super Mario
https://github.com/anthonytopper/rl-final/tree/super-mario-bros-dqn

https://github.com/anthonytopper/rl-final/tree/super-mario-smooth

## Videos
https://www.youtube.com/playlist?list=PL-RcCbVagfNi1WgbXUnBzrBC2AzS-hc83
